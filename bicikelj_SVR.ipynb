{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data and DateTime index preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/bicikelj_train.csv\")\n",
    "# Convert the \"timespamp\" colum to a datetime object and set it as the index\n",
    "df = df.set_index(\"timestamp\")\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Count the number of duplicate index values\n",
    "display(df.index.duplicated().sum())\n",
    "\n",
    "display(df.head())\n",
    "display(df.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into multiple dataframes, each with one station\n",
    "df_list = [df[[station]] for station in df.columns]\n",
    "df_list[0].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution of one station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig, ax = plt.subplots(figsize=(20, 5))\n",
    "df_list[0].plot(ax=ax, label='Training Set', title=f\"Visualization for {df_list[0].columns[0]} station\", style=\".\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Cross Validation Example and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = TimeSeriesSplit(n_splits=10, gap=(60 // 5))\n",
    "df_list = [station.sort_index() for station in df_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(10, 1, figsize=(15, 15), sharex=True)\n",
    "\n",
    "fold = 0\n",
    "for train_idx, val_idx in tss.split(df_list[0]):\n",
    "    train = df_list[0].iloc[train_idx]\n",
    "    test = df_list[0].iloc[val_idx]\n",
    "    train[df_list[0].columns[0]].plot(ax=axs[fold],\n",
    "                          label='Training Set',\n",
    "                          title=f'Data Train/Test Split Fold {fold}')\n",
    "    test[df_list[0].columns[0]].plot(ax=axs[fold],\n",
    "                         label='Test Set')\n",
    "    axs[fold].axvline(test.index.min(), color='black', ls='--')\n",
    "    fold += 1\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features created from datetime index (hour, dayofweek, month, dayofyear, dayofmonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(station):\n",
    "    \"\"\"\n",
    "    Create time series features based on time series index.\n",
    "    \"\"\"\n",
    "    station = station.copy()\n",
    "    station['hour'] = station.index.hour\n",
    "    station['dayofweek'] = station.index.dayofweek\n",
    "    station['month'] = station.index.month\n",
    "    station['dayofyear'] = station.index.dayofyear\n",
    "    station['dayofmonth'] = station.index.day\n",
    "    return station\n",
    "\n",
    "df_list = [create_features(station) for station in df_list]\n",
    "df_list[0].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag features (60 and 120 min lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lags(station):\n",
    "    station = station.copy()\n",
    "    # Create 60, 90 and 120 minutes lags\n",
    "    # lags = [60, 90, 120]\n",
    "    lags = [60, 120]\n",
    "    for lag in lags:\n",
    "        df_lag = station.copy()\n",
    "        # Drop all but the target column\n",
    "        df_lag = df_lag[[station.columns[0]]]\n",
    "        # Subtract 60 minutes from the index\n",
    "        df_lag.index = df_lag.index + pd.Timedelta(minutes=lag)\n",
    "        # Rename the column to \"lag_60\"\n",
    "        df_lag.columns = [f\"lag_{lag}\"]\n",
    "        # Merge the dataframe with the lagged dataframe\n",
    "        station = pd.merge_asof(\n",
    "            station,\n",
    "            df_lag,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            direction=\"nearest\",\n",
    "            tolerance=pd.Timedelta(\"15m\"),\n",
    "        )\n",
    "    return station\n",
    "\n",
    "df_list = [add_lags(station) for station in df_list]\n",
    "display(df_list[0].tail())\n",
    "display(df_list[0].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0].to_csv(\"data/other/processed.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## School day feature (0 for holidays and weekends, 1 for school days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_school_days(station):\n",
    "    # Add vacation feature which is 1 if the day is a school vacation day and 0 otherwise\n",
    "    # School days are from 1.9.2022 to 31.10.2022 without weekends\n",
    "    station[\"vacation\"] = 0\n",
    "    station.loc[(station.index >= \"2022-09-01\") & (station.index <= \"2022-10-31\") & (station.index.dayofweek < 5), \"vacation\"] = 1\n",
    "    return station\n",
    "\n",
    "df_list = [add_school_days(station) for station in df_list]\n",
    "display(df_list[0].tail())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather feature (Amount of rain in mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(\"data/extra/padavine_polurno.csv\")\n",
    "weather = weather.drop(columns=[\"station_id\", \"station_name\"])\n",
    "weather[\"datum\"] = pd.to_datetime(weather[\"datum\"])\n",
    "weather = weather.set_index(\"datum\")\n",
    "weather.info()\n",
    "\n",
    "def add_weather(df, weather):\n",
    "    df = df.copy()\n",
    "    weather = weather.copy()\n",
    "    # Add weather features by matrhing the Date, Hour of the day and the station\n",
    "    return pd.merge_asof(\n",
    "        df,\n",
    "        weather,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        direction=\"nearest\",\n",
    "    )\n",
    "\n",
    "df_list = [add_weather(station, weather) for station in df_list]\n",
    "display(df_list[0].tail())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free spaces feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"data/extra/bicikelj_metadata.csv\")\n",
    "\n",
    "def free_spaces(station):\n",
    "    station = station.copy()\n",
    "    # Get the total number of spaces for the station\n",
    "    station_metadata = metadata[metadata[\"postaja\"] == station.columns[0]]\n",
    "    total_space = station_metadata[\"total_space\"].values[0]\n",
    "    lags = [60, 120]\n",
    "    for lag in lags:\n",
    "        station_lag = station.copy()\n",
    "        # Drop all but the target column\n",
    "        station_lag = station_lag[[station.columns[0]]]\n",
    "        # Subtract the number of occupied spaces from the total number of spaces\n",
    "        station_lag[station.columns[0]] = total_space - station_lag[station.columns[0]]\n",
    "        # Subtract 60 minutes from the index\n",
    "        station_lag.index = station_lag.index + pd.Timedelta(minutes=lag)\n",
    "        # Rename the column to \"lag_60\"\n",
    "        station_lag.columns = [f\"free_space_lag_{lag}\"]\n",
    "        # Merge the dataframe with the lagged dataframe\n",
    "        station = pd.merge_asof(\n",
    "            station,\n",
    "            station_lag,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            direction=\"nearest\",\n",
    "            tolerance=pd.Timedelta(\"15m\"),\n",
    "        )\n",
    "    return station\n",
    "\n",
    "df_list = [free_spaces(station) for station in df_list]\n",
    "display(df_list[0].tail())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closest stations feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_closest_station_lags(station1, station2, index):\n",
    "    station1 = station1.copy()\n",
    "    station2 = station2.copy()\n",
    "    # Create 60, 90 and 120 minutes lags\n",
    "    # lags = [60, 90, 120]\n",
    "    lags = [60, 120]\n",
    "    for lag in lags:\n",
    "        station2_lag = station2.copy()\n",
    "        # Drop all but the target column\n",
    "        station2_lag = station2_lag[[station2.columns[0]]]\n",
    "        # Subtract 60 minutes from the index\n",
    "        station2_lag.index = station2_lag.index + pd.Timedelta(minutes=lag)\n",
    "        # Rename the column to \"lag_60\"\n",
    "        station2_lag.columns = [f\"station_{index}_lag_{lag}\"]\n",
    "        # Merge the dataframe with the lagged dataframe\n",
    "        station1 = pd.merge_asof(\n",
    "            station1,\n",
    "            station2_lag,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            direction=\"nearest\",\n",
    "            tolerance=pd.Timedelta(\"15m\"),\n",
    "        )\n",
    "    return station1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "number_of_closest_stations = 5\n",
    "def add_n_closest(station, df, metadata, n=5):\n",
    "    station = station.copy()\n",
    "    metadata = metadata.copy()\n",
    "    metadata = metadata.copy()\n",
    "    # Get the row where the \"postaja\" column matches the station name\n",
    "    station_metadata = metadata[metadata[\"postaja\"] == station.columns[0]]\n",
    "    longitude, latitude = station_metadata[\"geo-visina\"].values[0], station_metadata[\"geo-sirina\"].values[0]\n",
    "\n",
    "    # Add the distance from the station to every other station\n",
    "    metadata[\"razdralja\"] = np.sqrt((metadata[\"geo-visina\"] - longitude) ** 2 + (metadata[\"geo-sirina\"] - latitude) ** 2)\n",
    "    # Sort the dataframe by the distance\n",
    "    closest_stations_names = metadata.sort_values(\"razdralja\").iloc[1:n + 1][\"postaja\"].values\n",
    "    # Get the dataframes of the closest stations\n",
    "    closest_stations_df = [pd.DataFrame(df[df.columns[0]]) for df in df if df.columns[0] in closest_stations_names]\n",
    "    # Add the closest station lags to the dataframe\n",
    "    for index, closest_station in enumerate(closest_stations_df):\n",
    "        station = add_closest_station_lags(station, closest_station, index + 1)\n",
    "    return station\n",
    "\n",
    "df_list = [add_n_closest(station, df_list, metadata, number_of_closest_stations) for station in df_list]\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (OneHotEncoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot encode the categorical features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "# Define the categorical features\n",
    "categoricals = [\"hour\", \"dayofweek\", \"month\"]\n",
    "# Fit the encoder on the categorical features only\n",
    "encoder.fit(df_list[0][categoricals])\n",
    "def one_hot_encode(df, encoder=encoder, features=categoricals):\n",
    "    df = df.copy()\n",
    "    # Encode the categorical features\n",
    "    encoded = pd.DataFrame(encoder.transform(df[features]))\n",
    "    encoded.columns = encoder.get_feature_names_out(features)\n",
    "    encoded.index = df.index\n",
    "    # Concatenate the original dataframe and the encoded dataframe\n",
    "    df = pd.concat([df, encoded], axis=1)\n",
    "    # Drop the original categorical columns\n",
    "    df = df.drop(features, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the one hot encoder on all dataframes\n",
    "df_list = [one_hot_encode(station) for station in df_list]\n",
    "\n",
    "display(df_list[0].tail())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into 60 min and 120 min lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From the dataframes make two dataframes, one with all the features and 60 minutes lag and one with 120 minutes lag\n",
    "df_list_60 = [station.drop(columns=[\"lag_120\", \"free_space_lag_120\"]) for station in df_list]\n",
    "for i in range(number_of_closest_stations):\n",
    "    df_list_60 = [station.drop(columns=[f\"station_{i+1}_lag_120\"]) for station in df_list_60]\n",
    "df_list_120 = [station.drop(columns=[\"lag_60\", \"free_space_lag_60\"]) for station in df_list]\n",
    "for i in range(number_of_closest_stations):\n",
    "    df_list_120 = [station.drop(columns=[f\"station_{i+1}_lag_60\"]) for station in df_list_120]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing With Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_testing(df_list, params=None):\n",
    "        tss = TimeSeriesSplit(n_splits=5, gap=(60 // 5))\n",
    "        df_list = [station.sort_index() for station in df_list]\n",
    "        scores_list = []\n",
    "        preds_list = []\n",
    "        for station in df_list:\n",
    "                station.dropna(inplace=True)\n",
    "                preds = []\n",
    "                scores = []\n",
    "                for train_idx, val_idx in tss.split(station):\n",
    "                        train = station.iloc[train_idx]\n",
    "                        test = station.iloc[val_idx]\n",
    "\n",
    "                        FEATURES = train.columns[1:]\n",
    "                        TARGET = station.columns[0]\n",
    "\n",
    "                        X_train = train[FEATURES]\n",
    "                        y_train = train[TARGET]\n",
    "\n",
    "                        X_test = test[FEATURES]\n",
    "                        y_test = test[TARGET]\n",
    "\n",
    "                        model = SVR()\n",
    "                        \n",
    "                        model.fit(X_train, y_train)\n",
    "\n",
    "                        y_pred = model.predict(X_test)\n",
    "                        preds.append(y_pred)\n",
    "                        score = mean_absolute_error(y_test, y_pred)\n",
    "                        scores.append(score)\n",
    "                \n",
    "                preds_list.append(preds)\n",
    "                scores_list.append(scores)\n",
    "\n",
    "                print(f'{station.columns[0]}: Score across folds {np.mean(scores):0.4f}')\n",
    "                # print(f'{station.columns[0]}: Fold scores:{scores}')\n",
    "        return scores_list, preds_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the 60 min lag model before hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list_60, preds_list_60 = cross_validation_testing(df_list_60)\n",
    "print(f'Average Score: {np.mean(scores_list_60):0.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the 120 min lag model before hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list_120, preds_list_120 = cross_validation_testing(df_list_120)\n",
    "print(f'Average Score: {np.mean(scores_list_120):0.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "def hyperparameter_tuning(df_station):\n",
    "    df_station.dropna(inplace=True)\n",
    "    tss = TimeSeriesSplit(n_splits=5, gap=(60 // 5))\n",
    "    df_station = df_station.sort_index()\n",
    "    params = {\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'tol': [1e-3, 1e-4, 1e-2],\n",
    "        'C': [0.1, 1.0, 10.0, 100.0],\n",
    "        'epsilon': [0.1, 0.2, 0.05],\n",
    "        'shrinking': [True, False],\n",
    "            }\n",
    "    \n",
    "    FEATURES = df_station.columns[1:]\n",
    "    TARGET = df_station.columns[0]\n",
    "    SVR_reg = SVR()\n",
    "    # SVR_grid = GridSearchCV(SVR_reg, params, cv=tss, verbose=2, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "    SVR_grid = RandomizedSearchCV(SVR_reg, params, cv=tss, verbose=2, n_jobs=-1, scoring='neg_mean_absolute_error', n_iter=10, random_state=42)\n",
    "    SVR_grid.fit(df_station[FEATURES], df_station[TARGET])\n",
    "    print(f\"{df_station.columns[0]}: {SVR_grid.best_params_}\")\n",
    "    return SVR_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuning_60 = [hyperparameter_tuning(station) for station in df_list_60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuning_120 = [hyperparameter_tuning(station) for station in df_list_120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best parameters for each station in a dictionary\n",
    "best_params_60 = {station.columns[0]: model.best_params_ for station, model in zip(df_list_60, model_tuning_60)}\n",
    "best_params_120 = {station.columns[0]: model.best_params_ for station, model in zip(df_list_120, model_tuning_120)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best hyperparameters\n",
    "# Save the best parameters in a json file\n",
    "import pickle\n",
    "with open(\"hyperparameters/best_SVR_params_60.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_params_60, f)\n",
    "\n",
    "with open(\"hyperparameters/best_SVR_params_120.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_params_120, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the best parameters from the pkl file\n",
    "import pickle\n",
    "with open(\"hyperparameters/best_SVR_params_60.pkl\", \"rb\") as f:\n",
    "    best_params_60 = pickle.load(f)\n",
    "\n",
    "with open(\"hyperparameters/best_SVR_params_120.pkl\", \"rb\") as f:\n",
    "    best_params_120 = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on the whole dataset (One for each station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df_list, params):\n",
    "    models = []\n",
    "    for station in df_list:\n",
    "        station.dropna(inplace=True)\n",
    "\n",
    "        FEATURES = station.columns[1:]\n",
    "        TARGET = station.columns[0]\n",
    "        \n",
    "        X_all = station[FEATURES]\n",
    "        y_all = station[TARGET]\n",
    "        \n",
    "        reg = SVR(**params[station.columns[0]])\n",
    "        reg.fit(X_all, y_all)\n",
    "        \n",
    "        models.append(reg)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_60 = train_models(df_list_60, best_params_60)\n",
    "models_120 = train_models(df_list_120, best_params_120)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the test data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and preparing the test data\n",
    "df_test = pd.read_csv(\"data/bicikelj_test.csv\")\n",
    "df_test = df_test.set_index(\"timestamp\")\n",
    "df_test.index = pd.to_datetime(df_test.index)\n",
    "df_test = df_test.sort_index()\n",
    "\n",
    "# Create a list of dataframes, one for each station for the test and train set\n",
    "df_train_list = [df[[station]] for station in df.columns]\n",
    "df_test_list = [df_test[[station]] for station in df_test.columns]\n",
    "\n",
    "# Add the \"Test\" column to the dataframes\n",
    "df_train_list = [station.assign(Test=False) for station in df_train_list]\n",
    "df_test_list = [station.assign(Test=True) for station in df_test_list]\n",
    "\n",
    "test_df_list = []\n",
    "for train, test in zip(df_train_list, df_test_list):\n",
    "        # Concatenate the two dataframes\n",
    "        train_n_test = pd.concat([train, test], axis=0)\n",
    "        train_n_test = train_n_test.sort_index()\n",
    "        # Add the lags and datetime features\n",
    "        train_n_test = create_features(train_n_test)\n",
    "        train_n_test = add_lags(train_n_test)\n",
    "        train_n_test = add_school_days(train_n_test)\n",
    "        train_n_test = add_weather(train_n_test, weather)\n",
    "        train_n_test = free_spaces(train_n_test)\n",
    "        train_n_test = add_n_closest(train_n_test, df_list, metadata, n=5)\n",
    "        # One hot encode the categorical features\n",
    "        train_n_test = one_hot_encode(train_n_test)\n",
    "        # Select only the rows which are in the test set\n",
    "        station_test = train_n_test[train_n_test[\"Test\"] == True]\n",
    "        # Drop the \"Test\" column\n",
    "        station_test = station_test.drop(\"Test\", axis=1)\n",
    "        # Add the dataframe to the list\n",
    "        test_df_list.append(station_test)\n",
    "\n",
    "display(test_df_list[0].head())\n",
    "display(test_df_list[0].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of dataframes from rows where lag_60 is not null\n",
    "df_list_60 = [station[station[\"lag_60\"].notnull()] for station in test_df_list]\n",
    "# Remove the lag_120 columns \n",
    "df_list_60 = [station.drop(\"lag_120\", axis=1) for station in df_list_60]\n",
    "df_list_60 = [station.drop(\"free_space_lag_120\", axis=1) for station in df_list_60]\n",
    "for i in range(number_of_closest_stations):\n",
    "    df_list_60 = [station.drop(f\"station_{i+1}_lag_120\", axis=1) for station in df_list_60]\n",
    "\n",
    "# Make a list of dataframes from rows where lag_60 is null\n",
    "df_list_120 = [station[station[\"lag_60\"].isnull()] for station in test_df_list]\n",
    "# Remove the lag_60 columns\n",
    "df_list_120 = [station.drop(\"lag_60\", axis=1) for station in df_list_120]\n",
    "df_list_120 = [station.drop(\"free_space_lag_60\", axis=1) for station in df_list_120]\n",
    "for i in range(number_of_closest_stations):\n",
    "    df_list_120 = [station.drop(f\"station_{i+1}_lag_60\", axis=1) for station in df_list_120]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(models, df_list):\n",
    "    preds_list = []\n",
    "    for model, station in zip(models, df_list):\n",
    "        # Get the predictions for the station\n",
    "        pred = model.predict(station[station.columns[1:]])\n",
    "        # Transform the predictions to integers\n",
    "        # pred = pred.round(0).astype(np.int64)\n",
    "        # Add the predictions to the list\n",
    "        preds_list.append(pred)\n",
    "    return preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the predictions\n",
    "preds_list_60 = make_predictions(models_60, df_list_60)\n",
    "preds_list_120 = make_predictions(models_120, df_list_120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the predictions for 60 and 120 minutes into one list\n",
    "preds_list_merged = []\n",
    "for pred_60, pred_120 in zip(preds_list_60, preds_list_120):\n",
    "    pred = []\n",
    "    for i in range(len(pred_120)):\n",
    "        pred.append(pred_60[i])\n",
    "        pred.append(pred_120[i])\n",
    "    preds_list_merged.append(pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the predictions to the test set\n",
    "predicted = pd.read_csv(\"data/bicikelj_test.csv\")\n",
    "stations = predicted.columns[1:]\n",
    "for station, pred in zip(stations, preds_list_merged):\n",
    "        predicted[station] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change every negative value to 0\n",
    "for station in predicted.columns[1:]:\n",
    "    predicted[station] = predicted[station].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "# From each row of the metadata dataframe, get the \"postaja\" and the \"total_space\" and add them to a dictionary\n",
    "capacity_dict = {}\n",
    "for index, row in metadata.iterrows():\n",
    "    capacity_dict[row[\"postaja\"]] = row[\"total_space\"]\n",
    "\n",
    "# For each station check if the predicted number of bikes is greater than the capacity and if so, set it to the capacity\n",
    "for station in predicted.columns[1:]:\n",
    "    predicted[station] = predicted[station].apply(lambda x: capacity_dict[station] if x > capacity_dict[station] else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted.to_csv(\"data/predictions/predicted_SVR_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"models/models_SVR_60.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(models_60, f)\n",
    "\n",
    "# with open(\"models/models_SVR_120.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(models_120, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bicikelj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
